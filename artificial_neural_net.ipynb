{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "January 19, 2016 - Women in Data Science ATX Meetup - \"Data Science from Scratch\" Workshop\n",
    "# Artificial Neural Network\n",
    "\n",
    "Large neural networks (aka Deep Learning) are the current hot-topic in machine learning. In this workshop, you will learn to build and evaluate alternative neural network models using the MNIST handwritten digits dataset.\n",
    "\n",
    "\n",
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1  What is a Neural Net?\n",
    "*\"A neural network (aka multilayer perceptron) is a computing system made up of a number of simple, highly interconnected processing elements, which process information by their dynamic state response to external inputs.\"*\n",
    "\n",
    "\n",
    "![biological_neuron](images/biological_neuron.png)\n",
    "\n",
    "- Inspired by biological nervous systems\n",
    "- Works similar to human brain and acquires knowledge through learning by examples\n",
    "- Knowledge is stored within the interconnected strengths known as synaptic weight\n",
    "- Often referred to as a \"black box\" algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2  Advantages Over Conventional Techniques\n",
    "- Becoming increasingly popular because of massive dataset availibility and powerful computing \n",
    "- Remarkable ability to derive meaning from complicated or imprecise data\n",
    "- Extract patterns and detect trends that are too complex or noisy to be discovered\n",
    "- Applies to problems where the relationships may be quite dynamic or non-linear\n",
    "- Ability to adapt and learn in real-time (e.g. recurrent neural networks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3  How do they work?\n",
    "The simplest and most known neural network type is the __multilayer perceptron (MLP)__. It typically includes an *__input layer__*, one or several *__hidden layers__*, and an *__output layer__*.\n",
    "\n",
    "![layers](images/layers.png)\n",
    "\n",
    "- __Forward Propagation__ -- feeds a batch of inputs by applying weights and activation function for the next layer \n",
    "- __Activation Function__ -- sigmoid function output varies continuously but not linearly as the input changes. it bears greater resemblence to how neurons operate than linear activitation function\n",
    "\n",
    "\n",
    "![activation](images/activation.png)\n",
    "\n",
    "\n",
    "- __Backward Propagation__ -- calculates how the error changes as each weight is increased or decreased slightly then back-calculates the error associated with each unit from the preceding layer\n",
    "- __Learning/Optimization__ -- new weights are adjusted/updated based on the resulting error from previously used weights using gradient descent\n",
    "\n",
    "\n",
    "![gradient_descent](images/gradient_descent.png)\n",
    "\n",
    "\n",
    "- __Dropout__ -- is a technique addressing the common overfitting problems in large neural networks (deep learning). The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents overfitting and gives major improvements over other regularization methods.\n",
    "\n",
    "\n",
    "![dropout](images/dropout.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4  Simple Intuition with Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ]) # batch input X = (4x3)\n",
    "y = np.array([[0],[1],[1],[0]])                   # batch label y = (4x1)\n",
    "pd.DataFrame(np.concatenate((X,y),axis=1),columns=['x1','x2','x3','y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Activation Function\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Weight Initialization\n",
    "np.random.seed(1337) # for reproducibility\n",
    "w_01 = 2*np.random.random((3,4)) - 1\n",
    "w_12 = 2*np.random.random((4,1)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![forward_back](images/forward_back_prop.png)\n",
    "\n",
    "#### gradient descent review: https://github.com/JostineHo/neural-nets/blob/master/gradient_descent_math.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Learning through Multiple Iterations -- implements gradient descent to minimize cost function\n",
    "# for j in range(300): \n",
    "\n",
    "# Forward Propagation -- feeds a batch of inputs(batch size = 4, the number of examples in X)\n",
    "#                        by applying weights and activation function for the next layer \n",
    "layer1 = sigmoid(np.dot(X, w_01))\n",
    "layer2 = sigmoid(np.dot(layer1, w_12))\n",
    "\n",
    "# Backward Propagation -- calculates how the error changes as each weight is increased or decreased slightly\n",
    "#                         then back-calculates the error associated with each unit from the preceding layer \n",
    "layer2_delta = (y - layer2)*(layer2*(1-layer2))\n",
    "layer1_delta = layer2_delta.dot(w_12.T)*(layer1*(1-layer1))\n",
    "\n",
    "# Learning Step -- new weights are adjusted/updated based on the resulting error from previously used weights\n",
    "w_12 += layer1.T.dot(layer2_delta)\n",
    "w_01 += X.T.dot(layer1_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prediction -- final forward propagated output from using the latest learned weights \n",
    "layer1 = sigmoid(np.dot(X, w_01))\n",
    "layer2 = sigmoid(np.dot(layer1, w_12))\n",
    "# Softmax -- probablity output from sigmoid (without threshold round up to 1 or 0)\n",
    "yhat = layer2 \n",
    "pd.DataFrame(np.concatenate((yhat,y),axis=1),columns=['yhat','y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5  The MNIST Dataset\n",
    "![mnist](images/mnist.png)\n",
    "\n",
    "- MNIST is a database containing images of handwritten digits, with each **28 pixels by 28 pixels** image labeled by integer\n",
    "- This data has been preprocessed and organised to make it easy to use with various algorithms\n",
    "- The set __X_train__ contains __60,000 samples of training data__ with corresponding labels (y_train)\n",
    "- The set __X_test__ contains __10,000 samples of test data__ with corresponding labels (y_test)\n",
    "\n",
    "### Import Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __X_train__ is a tensor of shape (60000, 28, 28) containing, for each training grayscale image, the value of each pixel which is a int in [0;255]\n",
    "- __y_train__  is a tensor of shape (60000, 1) containing, for training test image, the label of each image which is an int in [0;9]\n",
    "- __X_test__  is a tensor of shape (10000, 28, 28) containing, for each test grayscale image, the value of each pixel which is a int in [0;255]\n",
    "- __y_test__  is a tensor of shape (10000, 1) containing, for each test image, the label of each image which is an int in [0;9]\n",
    "\n",
    "![x_train](images/X_train.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "print('X_train shape:', X_train.shape)\n",
    "# Each image in MNIST has a corresponding label, a number between 0 and 9 \n",
    "# representing the digit drawn in the image\n",
    "print('y_train shape:', y_train.shape) \n",
    "y_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape Image Data\n",
    "![pixels28x28](images/pixels28x28.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scale each entry in the tensor so that pixel intensity can be represented as a float between 0 and 1\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Flatten each 2D image data to an 1D array as model input (28*28=784)\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "# In this dataset, we will convert our labels to \"one-hot vectors\". A one-hot vector is\n",
    "# a vector which is 0 in most dimensions, and 1 in a single dimension. In this case, the\n",
    "# nth digit will be represented as a vector which is 1 in the nth dimension. \n",
    "nb_classes = 10 # One class per digit\n",
    "y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Sample Image and Label Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_n = 1\n",
    "plt.imshow(X_train[sample_n].reshape(28,28), cmap='binary')\n",
    "print('Corresponding Label: ', y_train[sample_n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6  Simple Multilayer Perceptron with Keras\n",
    "\n",
    "There are many relatively new libraries for using neural networks (Caffe, Theano, Torch ..etc), but there isn't yet a single standard. We will use Keras, which is arguably the most popular library for machine learning. [Keras](https://keras.io/) is a high level and easy to use library that allows us to build out our neural network in layers. \n",
    "\n",
    "\n",
    "### Import Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD, RMSprop, Adadelta\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now experiment with different neural network architectures (e.g. number and size of hidden layers), levels of dropout, and number of variables entering the network.\n",
    "\n",
    "### Define the model achitecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# Hidden Layer 1\n",
    "model.add(Dense(512, input_shape=(784,))) # --> 1st hidden layer requires specification of X input shape (1D array)\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "# Hidden Layer 2\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "# Output Layer \n",
    "model.add(Dense(nb_classes))     # --> outputs classification in the form of one-hot encoding vectors         \n",
    "model.add(Activation('softmax')) # --> score as probability for each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the network learn from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128 # Number of images used in each optimization step\n",
    "nb_classes = 10  # Number of different classes\n",
    "nb_epoch = 5     # Number of times the whole data is used to learn\n",
    "\n",
    "# Keras Optimizers:\n",
    "sgd = SGD()\n",
    "rms = RMSprop()\n",
    "ada = Adadelta()\n",
    "\n",
    "# The function to optimize is the cross entropy between the true label and the output (softmax) of the model\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adadelta', # Select an optimizer: 'sgd', 'rms', 'adadelta'\n",
    "              metrics=[\"accuracy\"])\n",
    " \n",
    "# Train the model\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size, \n",
    "          nb_epoch=nb_epoch,\n",
    "          verbose=2,\n",
    "          validation_data=(X_test, y_test))\n",
    " \n",
    "# Evaluate model performance on the test set\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    " \n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform several trainings to find the best architecture\n",
    "Every time you try a new network, write down your predictions for how this network's performance will compare to your previous networks' performance in terms of __execution speed__, __training error__, and __validation error__. This will give you a better sense of which parameters to tune in order to build the final model architecture for this particular dataset. Find trade off between accuracy and model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 7  Applications in Deep Learning\n",
    "\n",
    "### Pattern Recognition\n",
    "\n",
    "- [Self-Driving Cars](http://www.bloomberg.com/features/2015-george-hotz-self-driving-car/): A hobbyist is able to teach his car to self-drive in a few hours. \n",
    "- [Music Composition](http://web.mit.edu/felixsun/www/neural-music.html): Music can be composed based on different composer styles.\n",
    "- [Predict the Future](http://news.mit.edu/2016/teaching-machines-to-predict-the-future-0621): Well, that's the claim by these folks at MIT\n",
    "\n",
    "### Image Classification\n",
    "- [Location Identification from Photographs](https://www.technologyreview.com/s/600889/google-unveils-neural-network-with-superhuman-ability-to-determine-the-location-of-almost/): Google is able to identify the location of where a photograph is taken just my analyzing the scene.\n",
    "- [Face Identification](http://gitxiv.com/posts/fDJ7nHHou57aLEjBQ/the-megaface-benchmark-1-million-faces-for-recognition-at): Face recognition is so common that it is no longer surprising.\n",
    "\n",
    "### Natural Language Processing\n",
    "- [Realtime Speech Translation](http://blogs.skype.com/2014/12/15/skype-translator-how-it-works/): Microsoft Skype is able to translate voice into different languages in realtime. Something straight out of the universal translator in Star Trek. \n",
    "\n",
    "### More Applications\n",
    "http://www.deeplearningpatterns.com/doku.php/applications?utm_content=buffer98aea&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
